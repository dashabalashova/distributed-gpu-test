name: SLURM CI – build -> test -> push

on:
  push:
    branches: [ main ]
  workflow_dispatch:

permissions:
  contents: read
  packages: write
  id-token: write

env:
  IMAGE_NAME: ghcr.io/dashabalashova/distributed-gpu-test
  REMOTE_DIR: /root/distributed-gpu-test
  SLURM_WAIT_TIMEOUT_SECONDS: 600

jobs:
  build:
    name: Build image
    runs-on: [self-hosted, docker-host]
    outputs:
      image-tag: ${{ steps.set-tag.outputs.image_tag }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set IMAGE_TAG
        id: set-tag
        run: |
          IMAGE_TAG="v0.1.0"
          echo "IMAGE_TAG=$IMAGE_TAG" >> $GITHUB_ENV
          echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT

      # - name: Set up QEMU and buildx
      #   uses: docker/setup-buildx-action@v2

      # - name: Build docker image (load into local docker daemon)
      #   uses: docker/build-push-action@v5
      #   with:
      #     context: .
      #     file: ./Dockerfile
      #     push: false
      #     load: true
      #     tags: ${{ env.IMAGE_NAME }}:${{ steps.set-tag.outputs.image_tag }}

      # - name: Create enroot squashfs (.sqsh) from local Docker image
      #   run: |
      #     set -euo pipefail
      #     SQSH_NAME="distributed-gpu-test.sqsh"
      #     echo "Creating enroot squashfs: $SQSH_NAME from ${{ env.IMAGE_NAME }}:${{ steps.set-tag.outputs.image_tag }}"
      #     enroot import --output "$SQSH_NAME" "dockerd://${{ env.IMAGE_NAME }}:${{ steps.set-tag.outputs.image_tag }}"
      #     echo "SQSH_NAME=$SQSH_NAME" >> $GITHUB_ENV

      # - name: Ensure remote dir exists (on SLURM host)
      #   uses: appleboy/ssh-action@v0.1.7
      #   with:
      #     host: ${{ secrets.SLURM_HOST }}
      #     username: ${{ secrets.SLURM_USER }}
      #     key: ${{ secrets.SLURM_SSH_KEY }}
      #     port: ${{ secrets.SLURM_SSH_PORT }}
      #     script: |
      #       mkdir -p "${{ env.REMOTE_DIR }}"

      # - name: slurm_train.sbatch and train.py to SLURM host
      #   uses: appleboy/scp-action@v0.1.7
      #   with:
      #     host: ${{ secrets.SLURM_HOST }}
      #     username: ${{ secrets.SLURM_USER }}
      #     key: ${{ secrets.SLURM_SSH_KEY }}
      #     port: ${{ secrets.SLURM_SSH_PORT }}
      #     source: "slurm_train.sbatch,train.py"
      #     target: "${{ env.REMOTE_DIR }}"
      #     strip_components: 0

      # - name: Copy .sqsh to SLURM host with rsync (shows progress)
      #   env:
      #     SLURM_SSH_PORT: ${{ secrets.SLURM_SSH_PORT }}
      #     SLURM_SSH_KEY: ${{ secrets.SLURM_SSH_KEY }}
      #     SLURM_USER: ${{ secrets.SLURM_USER }}
      #     SLURM_HOST: ${{ secrets.SLURM_HOST }}
      #     REMOTE_DIR: ${{ env.REMOTE_DIR }}
      #   run: |
      #     set -euo pipefail
      #     mkdir -p ~/.ssh

      #     printf '%s\n' "$SLURM_SSH_KEY" > ~/.ssh/slurm_key
      #     chmod 600 ~/.ssh/slurm_key

      #     rsync -avP -e "ssh -i ~/.ssh/slurm_key -p "${{ secrets.SLURM_SSH_PORT }}" -o StrictHostKeyChecking=no" \
      #       distributed-gpu-test.sqsh \
      #       "${SLURM_USER}@${SLURM_HOST}:${REMOTE_DIR}"
      #   shell: bash

  test-on-slurm:
    name: Test on remote SLURM
    needs: build
    runs-on: [self-hosted, docker-host]
    steps:
      - id: run_teset
        name: Run remote test – submit slurm job and wait
        # id: run_teset
        uses: appleboy/ssh-action@v0.1.7
        with:
          host: ${{ secrets.SLURM_HOST }}
          username: ${{ secrets.SLURM_USER }}
          key: ${{ secrets.SLURM_SSH_KEY }}
          port: ${{ secrets.SLURM_SSH_PORT }}
          script: |
            #!/bin/bash
            set -euo pipefail
        
            REMOTE_DIR="${{ env.REMOTE_DIR }}"
            mkdir -p "$REMOTE_DIR"
            cd "$REMOTE_DIR"

            NODES=$(scontrol show nodes | grep -c '^NodeName')
            GPUS=$(scontrol show nodes | sed -n 's/.*CfgTRES=.*gres\/gpu=\([0-9][0-9]*\).*/\1/p' | head -n1)
            sed -e "s/^#SBATCH --nodes=.*/#SBATCH --nodes=${NODES}/" \
                -e "s/^#SBATCH --gres=gpu:.*/#SBATCH --gres=gpu:${GPUS}/" \
                slurm_train.sbatch > slurm_train.submit.sbatch
            SBATCH_OUT=$(sbatch --export=ALL,GPUS_PER_NODE="$GPUS",NNODES="$NODES" slurm_train.submit.sbatch)
            if [ -z "$SBATCH_OUT" ]; then
              echo "Failed to submit job; sbatch output: $SBATCH_OUT"
              exit 2
            fi
            JOB_ID=$(echo "$SBATCH_OUT" | awk '{print $NF}')
            echo "Submitted batch job $JOB_ID"
        
            START_TS=$(date +%s)
            TIMEOUT=${{ env.SLURM_WAIT_TIMEOUT_SECONDS }}
            if [ -z "$TIMEOUT" ]; then TIMEOUT=3600; fi
        
            STATUS_FILE="$PWD/job_status.txt"
        
            get_exit_code_from_sacct() {
              INFO=$(sacct -j "$JOB_ID" --format=JobID,State,ExitCode -n -P 2>/dev/null || true)
              if [ -z "$INFO" ]; then
                return 1
              fi
              LINE=$(echo "$INFO" | sed -n '1p' | tr -d '\r' || true)
              if [ -z "$LINE" ]; then
                return 1
              fi
              SACCT_EXIT=$(echo "$LINE" | awk -F'|' '{print $3}' | awk -F: '{print $1}')
              SACCT_STATE=$(echo "$LINE" | awk -F'|' '{print $2}')
              if [ -z "$SACCT_EXIT" ]; then
                return 1
              fi
              return 0
            }
        
            while true; do
              if squeue -j "$JOB_ID" -h -o "%T" >/dev/null 2>&1; then
                STATE=$(squeue -j "$JOB_ID" -h -o "%T" | tr -d '\r' || true)
              else
                STATE=""
              fi
        
              if [ -z "$STATE" ]; then
                echo "Job $JOB_ID missing from squeue — checking sacct / status file..."
                if command -v sacct >/dev/null 2>&1; then
                  if get_exit_code_from_sacct; then
                    echo "sacct reports state=${SACCT_STATE}, exit=${SACCT_EXIT}"
                    if [ "$SACCT_EXIT" = "0" ]; then
                      exit 0
                    else
                      exit "$SACCT_EXIT"
                    fi
                  else
                    echo "sacct present but no ExitCode found; will wait shortly for $STATUS_FILE"
                  fi
                else
                  echo "sacct not available; will wait shortly for $STATUS_FILE"
                fi
        
                STATUS_WAIT=${{ env.SLURM_STATUS_WAIT_SECONDS }}
                if [ -z "$STATUS_WAIT" ]; then
                  STATUS_WAIT=180
                fi
                POLL_INTERVAL=5
                WAIT_START_TS=$(date +%s)
                echo "Waiting up to ${STATUS_WAIT}s for $STATUS_FILE to appear (poll every ${POLL_INTERVAL}s)..."
                while [ $(( $(date +%s) - WAIT_START_TS )) -lt "$STATUS_WAIT" ]; do
                  if [ -f "$STATUS_FILE" ]; then
                    STATUS=$(cat "$STATUS_FILE" | tr -d '\r' || true)
                    echo "Found $STATUS_FILE -> '$STATUS'"
                    case "$STATUS" in
                      success|0) exit 0 ;;
                      fail|1) echo "Remote job reported fail"; exit 1 ;;
                      *) echo "Unknown status file content: '$STATUS' — treating as failure"; exit 1 ;;
                    esac
                  fi
                  sleep $POLL_INTERVAL
                done
        
                echo "$STATUS_FILE did not appear after ${STATUS_WAIT}s — treating as failure"
                exit 1
              fi
        
              NOW_TS=$(date +%s)
              if [ $((NOW_TS - START_TS)) -gt "$TIMEOUT" ]; then
                echo "Timeout reached (${TIMEOUT}s), cancelling job $JOB_ID"
                scancel "$JOB_ID" || true
                exit 124
              fi
        
              echo "Job $JOB_ID state: $STATE — sleeping 10s..."
              sleep 10
            done

      - id: set_slurm_status
        name: Set job output based on previous step
        if: ${{ success() }}
        run: |
          echo "slurm_status=success"
          echo "slurm_status=success" >> $GITHUB_OUTPUT

      - id: set_slurm_status_fail
        name: Set job output on failure
        if: ${{ failure() }}
        run: |
          echo "slurm_status=failed"
          echo "slurm_status=failed" >> $GITHUB_OUTPUT

      # - name: Debug show slurm_status from needs
      #     run: |
      #       echo "RAW needs.test-on-slurm.outputs.slurm_status: >>>${{ needs.test-on-slurm.outputs.slurm_status }}<<<"
      #       echo "as env var:"
      #       echo "SLURM_STATUS='${SLURM_STATUS:-}'"
      #       printf "len=%s\n" "${#SLURM_STATUS:-0}"
      #     env:
      #       SLURM_STATUS: ${{ needs.test-on-slurm.outputs.slurm_status }}
    outputs:
      slurm_status: ${{ steps.set_slurm_status.outputs.slurm_status }}
            
  publish:
    name: Publish image to GHCR (only if SLURM job success)
    needs: [build, test-on-slurm]
    # if: ${{ needs.test-on-slurm.result == 'success' }}

    runs-on: [self-hosted, docker-host]
    steps:
      - name: Debug show slurm_status from needs
        run: |
          # выводим raw-значение прямо из GitHub expression (это разворачивается до запуска shell)
          echo "RAW needs.test-on-slurm.outputs.slurm_status: >>>${{ needs.test-on-slurm.outputs.slurm_status }}<<<"
    
          # присвоим в shell-переменную и почистим CR (если есть)
          SLURM_STATUS="${{ needs.test-on-slurm.outputs.slurm_status }}"
          SLURM_STATUS="${SLURM_STATUS//$'\r'/}"
    
          echo "as env var:"
          echo "SLURM_STATUS='${SLURM_STATUS}'"
          printf "len=%d\n" "${#SLURM_STATUS}"
        shell: bash

    if: ${{ needs['test-on-slurm'].outputs.slurm_status == 'success' }}
    # runs-on: [self-hosted, docker-host]
    env:
      IMAGE_NAME: ghcr.io/dashabalashova/distributed-gpu-test
      IMAGE_TAG: ${{ needs.build.outputs.image-tag }}
    steps:
      - name: Ensure image exists locally and push to GHCR
        env:
          CR_PAT: ${{ secrets.CR_PAT }}
        run: |
          set -euo pipefail
          TAG="${IMAGE_NAME}:${IMAGE_TAG}"
          if ! docker image inspect "$TAG" >/dev/null 2>&1; then
            echo "Image $TAG not found in local docker. Available images:"
            docker images --format 'table {{.Repository}}\t{{.Tag}}\t{{.ID}}\t{{.Size}}' || true
            exit 2
          fi
          echo "Logging in to ghcr.io"
          echo "${CR_PAT}" | docker login ghcr.io -u "${{ github.actor }}" --password-stdin
          echo "Pushing $TAG"
          # docker push "$TAG"